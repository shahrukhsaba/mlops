{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03. MLflow Experiment Tracking\n",
                "\n",
                "**MLOps Assignment - BITS Pilani (S1-25_AIMLCZG523)**\n",
                "\n",
                "---\n",
                "\n",
                "## Objectives (5 marks)\n",
                "1. **Integrate MLflow** for experiment tracking\n",
                "2. **Log Parameters** - Hyperparameters for each model\n",
                "3. **Log Metrics** - Accuracy, Precision, Recall, F1, ROC-AUC\n",
                "4. **Log Artifacts** - Confusion matrix, ROC curves, models\n",
                "5. **Compare Experiments** - Analyze multiple runs\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'pandas'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import required libraries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
                        "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
                    ]
                }
            ],
            "source": [
                "# Import required libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# MLflow\n",
                "import mlflow\n",
                "import mlflow.sklearn\n",
                "\n",
                "# Scikit-learn\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, precision_score, recall_score, f1_score,\n",
                "    roc_auc_score, confusion_matrix, roc_curve\n",
                ")\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"‚úÖ Libraries loaded successfully!\")\n",
                "print(f\"MLflow version: {mlflow.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup MLflow Tracking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup MLflow tracking URI (local file-based tracking)\n",
                "# Get the project root directory (parent of notebooks folder)\n",
                "PROJECT_ROOT = os.path.dirname(os.path.abspath(os.getcwd())) if os.getcwd().endswith('notebooks') else os.getcwd()\n",
                "\n",
                "# Change to project root for correct relative paths\n",
                "os.chdir(PROJECT_ROOT)\n",
                "print(f\"Working directory: {os.getcwd()}\")\n",
                "\n",
                "# Setup MLflow with file:// URI\n",
                "mlflow_dir = os.path.join(PROJECT_ROOT, 'mlruns')\n",
                "os.makedirs(mlflow_dir, exist_ok=True)\n",
                "mlflow.set_tracking_uri(f\"file://{mlflow_dir}\")\n",
                "\n",
                "# Create experiment\n",
                "EXPERIMENT_NAME = \"heart_disease_classification\"\n",
                "mlflow.set_experiment(EXPERIMENT_NAME)\n",
                "\n",
                "# Disable autologging to avoid conflicts in notebook\n",
                "mlflow.sklearn.autolog(disable=True)\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"MLFLOW TRACKING SETUP\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"Tracking URI: file://{mlflow_dir}\")\n",
                "print(f\"Experiment: {EXPERIMENT_NAME}\")\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load and Prepare Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data (using absolute path based on PROJECT_ROOT)\n",
                "data_path = os.path.join(PROJECT_ROOT, 'data/processed/heart_disease_clean.csv')\n",
                "print(f\"Loading data from: {data_path}\")\n",
                "df = pd.read_csv(data_path)\n",
                "\n",
                "# Define features\n",
                "NUMERICAL_FEATURES = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
                "CATEGORICAL_FEATURES = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
                "\n",
                "# Split features and target\n",
                "X = df.drop('target', axis=1)\n",
                "y = df['target']\n",
                "\n",
                "# Train-test split\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "# Build preprocessing pipeline\n",
                "preprocessor = ColumnTransformer([\n",
                "    ('num', StandardScaler(), NUMERICAL_FEATURES),\n",
                "    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), CATEGORICAL_FEATURES)\n",
                "])\n",
                "\n",
                "# Create screenshots directory\n",
                "SCREENSHOTS_DIR = os.path.join(PROJECT_ROOT, 'screenshots')\n",
                "os.makedirs(SCREENSHOTS_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"Training samples: {len(X_train)}\")\n",
                "print(f\"Test samples: {len(X_test)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Experiment 1: Logistic Regression with MLflow Tracking\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# EXPERIMENT 1: Logistic Regression\n",
                "with mlflow.start_run(run_name=\"LogisticRegression_Experiment\") as run:\n",
                "    \n",
                "    # Log tags\n",
                "    mlflow.set_tags({\n",
                "        \"model_type\": \"LogisticRegression\",\n",
                "        \"dataset\": \"heart_disease_uci\",\n",
                "        \"phase\": \"experiment\"\n",
                "    })\n",
                "    \n",
                "    # Define hyperparameters\n",
                "    params = {\n",
                "        'C': 1.0,\n",
                "        'penalty': 'l2',\n",
                "        'solver': 'lbfgs',\n",
                "        'max_iter': 1000\n",
                "    }\n",
                "    \n",
                "    # Log parameters\n",
                "    mlflow.log_params(params)\n",
                "    print(\"‚úÖ Parameters logged:\")\n",
                "    for k, v in params.items():\n",
                "        print(f\"   {k}: {v}\")\n",
                "    \n",
                "    # Create pipeline\n",
                "    lr_pipeline = Pipeline([\n",
                "        ('preprocessor', preprocessor),\n",
                "        ('classifier', LogisticRegression(**params, random_state=42))\n",
                "    ])\n",
                "    \n",
                "    # Train model\n",
                "    lr_pipeline.fit(X_train, y_train)\n",
                "    \n",
                "    # Predictions\n",
                "    y_pred = lr_pipeline.predict(X_test)\n",
                "    y_prob = lr_pipeline.predict_proba(X_test)[:, 1]\n",
                "    \n",
                "    # Calculate metrics\n",
                "    metrics = {\n",
                "        'accuracy': accuracy_score(y_test, y_pred),\n",
                "        'precision': precision_score(y_test, y_pred),\n",
                "        'recall': recall_score(y_test, y_pred),\n",
                "        'f1_score': f1_score(y_test, y_pred),\n",
                "        'roc_auc': roc_auc_score(y_test, y_prob)\n",
                "    }\n",
                "    \n",
                "    # Log metrics\n",
                "    mlflow.log_metrics(metrics)\n",
                "    print(\"\\n‚úÖ Metrics logged:\")\n",
                "    for k, v in metrics.items():\n",
                "        print(f\"   {k}: {v:.4f}\")\n",
                "    \n",
                "    # Create confusion matrix plot (save locally then log)\n",
                "    fig, ax = plt.subplots(figsize=(6, 5))\n",
                "    cm = confusion_matrix(y_test, y_pred)\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
                "    ax.set_title('Confusion Matrix - Logistic Regression')\n",
                "    ax.set_ylabel('True Label')\n",
                "    ax.set_xlabel('Predicted Label')\n",
                "    plt.tight_layout()\n",
                "    lr_cm_path = os.path.join(SCREENSHOTS_DIR, 'lr_confusion_matrix.png')\n",
                "    fig.savefig(lr_cm_path, dpi=150)\n",
                "    mlflow.log_artifact(lr_cm_path)\n",
                "    plt.close()\n",
                "    print(\"\\n‚úÖ Artifact logged: lr_confusion_matrix.png\")\n",
                "    \n",
                "    # Create ROC curve (save locally then log)\n",
                "    fig, ax = plt.subplots(figsize=(6, 5))\n",
                "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
                "    ax.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {metrics[\"roc_auc\"]:.3f})')\n",
                "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
                "    ax.set_xlabel('False Positive Rate')\n",
                "    ax.set_ylabel('True Positive Rate')\n",
                "    ax.set_title('ROC Curve - Logistic Regression')\n",
                "    ax.legend(loc='lower right')\n",
                "    ax.grid(True, alpha=0.3)\n",
                "    plt.tight_layout()\n",
                "    lr_roc_path = os.path.join(SCREENSHOTS_DIR, 'lr_roc_curve.png')\n",
                "    fig.savefig(lr_roc_path, dpi=150)\n",
                "    mlflow.log_artifact(lr_roc_path)\n",
                "    plt.close()\n",
                "    print(\"‚úÖ Artifact logged: lr_roc_curve.png\")\n",
                "    \n",
                "    # Log model\n",
                "    mlflow.sklearn.log_model(lr_pipeline, \"model\")\n",
                "    print(\"‚úÖ Model logged\")\n",
                "    \n",
                "    lr_run_id = run.info.run_id\n",
                "    print(f\"\\nüìù Run ID: {lr_run_id}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Experiment 2: Random Forest with MLflow Tracking\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# EXPERIMENT 2: Random Forest\n",
                "with mlflow.start_run(run_name=\"RandomForest_Experiment\") as run:\n",
                "    \n",
                "    # Log tags\n",
                "    mlflow.set_tags({\n",
                "        \"model_type\": \"RandomForest\",\n",
                "        \"dataset\": \"heart_disease_uci\",\n",
                "        \"phase\": \"experiment\"\n",
                "    })\n",
                "    \n",
                "    # Define hyperparameters\n",
                "    params = {\n",
                "        'n_estimators': 200,\n",
                "        'max_depth': 10,\n",
                "        'min_samples_split': 5,\n",
                "        'min_samples_leaf': 1\n",
                "    }\n",
                "    \n",
                "    # Log parameters\n",
                "    mlflow.log_params(params)\n",
                "    print(\"‚úÖ Parameters logged:\")\n",
                "    for k, v in params.items():\n",
                "        print(f\"   {k}: {v}\")\n",
                "    \n",
                "    # Create pipeline\n",
                "    rf_pipeline = Pipeline([\n",
                "        ('preprocessor', preprocessor),\n",
                "        ('classifier', RandomForestClassifier(**params, random_state=42))\n",
                "    ])\n",
                "    \n",
                "    # Train model\n",
                "    rf_pipeline.fit(X_train, y_train)\n",
                "    \n",
                "    # Predictions\n",
                "    y_pred = rf_pipeline.predict(X_test)\n",
                "    y_prob = rf_pipeline.predict_proba(X_test)[:, 1]\n",
                "    \n",
                "    # Calculate metrics\n",
                "    metrics = {\n",
                "        'accuracy': accuracy_score(y_test, y_pred),\n",
                "        'precision': precision_score(y_test, y_pred),\n",
                "        'recall': recall_score(y_test, y_pred),\n",
                "        'f1_score': f1_score(y_test, y_pred),\n",
                "        'roc_auc': roc_auc_score(y_test, y_prob)\n",
                "    }\n",
                "    \n",
                "    # Log metrics\n",
                "    mlflow.log_metrics(metrics)\n",
                "    print(\"\\n‚úÖ Metrics logged:\")\n",
                "    for k, v in metrics.items():\n",
                "        print(f\"   {k}: {v:.4f}\")\n",
                "    \n",
                "    # Create confusion matrix (save locally then log)\n",
                "    fig, ax = plt.subplots(figsize=(6, 5))\n",
                "    cm = confusion_matrix(y_test, y_pred)\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=ax)\n",
                "    ax.set_title('Confusion Matrix - Random Forest')\n",
                "    ax.set_ylabel('True Label')\n",
                "    ax.set_xlabel('Predicted Label')\n",
                "    plt.tight_layout()\n",
                "    rf_cm_path = os.path.join(SCREENSHOTS_DIR, 'rf_confusion_matrix.png')\n",
                "    fig.savefig(rf_cm_path, dpi=150)\n",
                "    mlflow.log_artifact(rf_cm_path)\n",
                "    plt.close()\n",
                "    print(\"\\n‚úÖ Artifact logged: rf_confusion_matrix.png\")\n",
                "    \n",
                "    # Create ROC curve (save locally then log)\n",
                "    fig, ax = plt.subplots(figsize=(6, 5))\n",
                "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
                "    ax.plot(fpr, tpr, 'g-', linewidth=2, label=f'ROC (AUC = {metrics[\"roc_auc\"]:.3f})')\n",
                "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
                "    ax.set_xlabel('False Positive Rate')\n",
                "    ax.set_ylabel('True Positive Rate')\n",
                "    ax.set_title('ROC Curve - Random Forest')\n",
                "    ax.legend(loc='lower right')\n",
                "    ax.grid(True, alpha=0.3)\n",
                "    plt.tight_layout()\n",
                "    rf_roc_path = os.path.join(SCREENSHOTS_DIR, 'rf_roc_curve.png')\n",
                "    fig.savefig(rf_roc_path, dpi=150)\n",
                "    mlflow.log_artifact(rf_roc_path)\n",
                "    plt.close()\n",
                "    print(\"‚úÖ Artifact logged: rf_roc_curve.png\")\n",
                "    \n",
                "    # Log model\n",
                "    mlflow.sklearn.log_model(rf_pipeline, \"model\")\n",
                "    print(\"‚úÖ Model logged\")\n",
                "    \n",
                "    rf_run_id = run.info.run_id\n",
                "    print(f\"\\nüìù Run ID: {rf_run_id}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. MLflow Tracking Summary\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SUMMARY\n",
                "print(\"=\" * 70)\n",
                "print(\"               MLFLOW EXPERIMENT TRACKING SUMMARY\")\n",
                "print(\"=\" * 70)\n",
                "print(\"\"\"\n",
                "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "‚îÇ                    WHAT WE LOGGED TO MLFLOW                         ‚îÇ\n",
                "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
                "‚îÇ  ‚úÖ PARAMETERS:                                                      ‚îÇ\n",
                "‚îÇ     - Model hyperparameters (C, penalty, n_estimators, etc.)        ‚îÇ\n",
                "‚îÇ     - Training configuration                                         ‚îÇ\n",
                "‚îÇ                                                                      ‚îÇ\n",
                "‚îÇ  ‚úÖ METRICS:                                                         ‚îÇ\n",
                "‚îÇ     - Accuracy, Precision, Recall, F1-Score, ROC-AUC                ‚îÇ\n",
                "‚îÇ     - Logged for each experiment run                                 ‚îÇ\n",
                "‚îÇ                                                                      ‚îÇ\n",
                "‚îÇ  ‚úÖ ARTIFACTS:                                                       ‚îÇ\n",
                "‚îÇ     - Confusion matrix plots (PNG)                                   ‚îÇ\n",
                "‚îÇ     - ROC curve plots (PNG)                                          ‚îÇ\n",
                "‚îÇ     - Trained model files (pickle)                                   ‚îÇ\n",
                "‚îÇ                                                                      ‚îÇ\n",
                "‚îÇ  ‚úÖ TAGS:                                                            ‚îÇ\n",
                "‚îÇ     - model_type, dataset, phase                                     ‚îÇ\n",
                "‚îÇ                                                                      ‚îÇ\n",
                "‚îÇ  üìÅ MLflow Tracking Directory: mlruns/                               ‚îÇ\n",
                "‚îÇ  üåê To view UI: mlflow ui --backend-store-uri ./mlruns              ‚îÇ\n",
                "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "\"\"\")\n",
                "print(\"‚úÖ Experiment Tracking with MLflow COMPLETE!\")\n",
                "print(\"=\" * 70)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Query all runs from the experiment\n",
                "client = mlflow.tracking.MlflowClient()\n",
                "experiment = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
                "\n",
                "if experiment:\n",
                "    runs = client.search_runs(\n",
                "        experiment_ids=[experiment.experiment_id],\n",
                "        order_by=[\"metrics.roc_auc DESC\"]\n",
                "    )\n",
                "    \n",
                "    print(\"=\" * 70)\n",
                "    print(\"                    MLFLOW EXPERIMENT RUNS\")\n",
                "    print(\"=\" * 70)\n",
                "    \n",
                "    comparison_data = []\n",
                "    for run in runs[:10]:  # Show top 10 runs\n",
                "        run_data = {\n",
                "            'Run Name': run.data.tags.get('mlflow.runName', 'N/A'),\n",
                "            'Model Type': run.data.tags.get('model_type', 'N/A'),\n",
                "            'Accuracy': run.data.metrics.get('accuracy', 0),\n",
                "            'Precision': run.data.metrics.get('precision', 0),\n",
                "            'Recall': run.data.metrics.get('recall', 0),\n",
                "            'F1-Score': run.data.metrics.get('f1_score', 0),\n",
                "            'ROC-AUC': run.data.metrics.get('roc_auc', 0),\n",
                "        }\n",
                "        comparison_data.append(run_data)\n",
                "    \n",
                "    comparison_df = pd.DataFrame(comparison_data)\n",
                "    print(comparison_df.to_string(index=False))\n",
                "    print(\"=\" * 70)\n",
                "else:\n",
                "    print(\"No experiment found!\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
