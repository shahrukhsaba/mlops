# Heart Disease Prediction - MLOps Assignment

[![CI/CD Pipeline](https://github.com/shahrukhsaba/mlops/actions/workflows/ci-cd.yml/badge.svg)](https://github.com/shahrukhsaba/mlops/actions)
[![Python 3.10](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-ready-blue.svg)](https://www.docker.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**BITS Pilani - MLOps Assignment (S1-25_AIMLCZG523)**

A production-ready MLOps pipeline for predicting heart disease risk using the UCI Heart Disease dataset. This project demonstrates end-to-end ML model development, CI/CD, containerization, and cloud deployment.

---

## üöÄ Quick Start - Run the Complete Project

```bash
# Step 1: Clone and navigate to the project
git clone https://github.com/shahrukhsaba/mlops.git
cd mlops

# Step 2: Install dependencies
pip install -r requirements.txt

# Step 3: Download and prepare data
python scripts/download_data.py

# Step 4: Run notebooks (generates EDA, training outputs & screenshots)
python scripts/execute_notebooks.py

# Step 5: Train the model for production
python scripts/train_and_save_locally.py

# Step 6: Build Docker image
docker build -t heart-disease-api:latest .

# Step 7: Run Docker container
docker run -d --name heart-disease-api -p 8000:8000 heart-disease-api:latest

# Step 8: Test the API
curl http://localhost:8000/health
curl -X POST "http://localhost:8000/predict?age=63&sex=1&cp=3&trestbps=145&chol=233&fbs=1&restecg=0&thalach=150&exang=0&oldpeak=2.3&slope=0&ca=0&thal=1"

# Step 9: Open API documentation
open http://localhost:8000/docs
```

---

## üìã Table of Contents

- [Quick Start](#-quick-start---run-the-complete-project)
- [Overview](#-overview)
- [Features](#-features)
- [Project Structure](#-project-structure)
- [Detailed Execution Steps](#-detailed-execution-steps)
- [Data Acquisition](#-data-acquisition)
- [Model Packaging & Reproducibility](#-model-packaging--reproducibility)
- [Model Training](#-model-training)
- [API Usage](#-api-usage)
- [Docker Deployment](#-docker-deployment)
- [Kubernetes Deployment](#-kubernetes-deployment)
- [CI/CD Pipeline](#-cicd-pipeline)
- [Monitoring](#-monitoring)
- [Testing](#-testing)
- [MLflow Experiment Tracking](#-mlflow-experiment-tracking)

---

## üéØ Overview

This project builds a machine learning classifier to predict heart disease risk based on patient health data. It includes:

- **Data Pipeline**: Automated data ingestion, validation, and preprocessing
- **Model Training**: Logistic Regression and Random Forest with hyperparameter tuning
- **Experiment Tracking**: MLflow integration for logging parameters, metrics, and artifacts
- **API Serving**: FastAPI-based prediction service
- **Containerization**: Docker image for portable deployment
- **Orchestration**: Kubernetes manifests for production deployment
- **CI/CD**: GitHub Actions workflow for automated testing and deployment
- **Monitoring**: Prometheus metrics and logging

---

## ‚ú® Features

| Feature | Description |
|---------|-------------|
| üìä EDA | Comprehensive exploratory data analysis with visualizations |
| üîß Feature Engineering | Automated feature creation and preprocessing pipeline |
| ü§ñ Multiple Models | Logistic Regression & Random Forest with GridSearchCV |
| üìà Experiment Tracking | MLflow logging of parameters, metrics, and artifacts |
| üß™ Testing | Unit and integration tests with pytest |
| üê≥ Docker | Containerized API for portable deployment |
| ‚ò∏Ô∏è Kubernetes | Deployment manifests with HPA and health checks |
| üîÑ CI/CD | GitHub Actions for automated pipeline |
| üì° Monitoring | Prometheus metrics and structured logging |

---

## üìÅ Project Structure

```
heart-disease-mlops/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ ci-cd.yml              # GitHub Actions CI/CD pipeline
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ app.py                     # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ predictor.py               # Prediction logic
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py                 # Pydantic schemas
‚îÇ   ‚îú‚îÄ‚îÄ middleware/                # API middleware
‚îÇ   ‚îî‚îÄ‚îÄ monitoring/                # Prometheus metrics
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                       # Raw data files
‚îÇ   ‚îî‚îÄ‚îÄ processed/                 # Cleaned data
‚îú‚îÄ‚îÄ k8s/
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml            # Kubernetes deployment
‚îÇ   ‚îú‚îÄ‚îÄ configmap.yaml             # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ ingress.yaml               # Ingress rules
‚îÇ   ‚îî‚îÄ‚îÄ namespace.yaml             # Namespace definition
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ production/                # Production model artifacts
‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus/                # Prometheus configuration
‚îÇ   ‚îî‚îÄ‚îÄ grafana/                   # Grafana dashboards
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_eda.ipynb                          # Exploratory Data Analysis
‚îÇ   ‚îú‚îÄ‚îÄ 02_feature_engineering_modeling.ipynb # Feature Engineering & Model Training
‚îÇ   ‚îî‚îÄ‚îÄ 03_mlflow_experiments.ipynb           # MLflow Experiment Tracking
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ download_data.py           # Data download script
‚îÇ   ‚îú‚îÄ‚îÄ train_and_save_locally.py  # Local training script
‚îÇ   ‚îî‚îÄ‚îÄ execute_notebooks.py       # Run all notebooks with outputs
‚îú‚îÄ‚îÄ screenshots/                    # Generated visualizations
‚îÇ   ‚îú‚îÄ‚îÄ 01_*.png                   # EDA screenshots
‚îÇ   ‚îú‚îÄ‚îÄ 02_*.png                   # Model training screenshots
‚îÇ   ‚îî‚îÄ‚îÄ 03_*.png                   # MLflow experiment screenshots
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data/                      # Data loading and preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ features/                  # Feature engineering
‚îÇ   ‚îú‚îÄ‚îÄ models/                    # Model training
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/                # Drift detection
‚îÇ   ‚îî‚îÄ‚îÄ tracking/                  # MLflow utilities
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/                      # Unit tests
‚îÇ   ‚îî‚îÄ‚îÄ integration/               # Integration tests
‚îú‚îÄ‚îÄ Dockerfile                     # Docker image definition
‚îú‚îÄ‚îÄ docker-compose.yml             # Multi-container setup
‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies
‚îî‚îÄ‚îÄ README.md                      # This file
```

---

## üìù Detailed Execution Steps

### Prerequisites

- Python 3.9+ or 3.10+
- Docker Desktop (for containerization)
- Git

### Step 1: Clone and Setup

```bash
# Clone the repository
git clone https://github.com/shahrukhsaba/mlops.git
cd mlops

# Create virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Step 2: Download Data

```bash
python scripts/download_data.py
```

**Expected Output:**
```
Dataset download and processing complete!
Final dataset shape: (297, 14)
Target distribution: {0: 160, 1: 137}
```

### Step 3: Run Notebooks (Optional - for EDA & Experiments)

```bash
# Execute all notebooks and generate outputs/screenshots
python scripts/execute_notebooks.py
```

**Expected Output:**
```
======================================================================
          EXECUTING NOTEBOOKS WITH OUTPUT CAPTURE
======================================================================

üìì Executing: notebooks/01_eda.ipynb
   ‚úÖ Executed and saved: notebooks/01_eda.ipynb

üìì Executing: notebooks/02_feature_engineering_modeling.ipynb
   ‚úÖ Executed and saved: notebooks/02_feature_engineering_modeling.ipynb

üìì Executing: notebooks/03_mlflow_experiments.ipynb
   ‚úÖ Executed and saved: notebooks/03_mlflow_experiments.ipynb
```

This will generate 13 screenshots in the `screenshots/` folder.

### Step 4: Train the Model

```bash
python scripts/train_and_save_locally.py
```

**Expected Output:**
```
Training Random Forest model for packaging...
Model saved to models/random_forest/model.pkl
```

### Step 5: Build Docker Image

```bash
docker build -t heart-disease-api:latest .
```

### Step 6: Run Docker Container

```bash
docker run -d --name heart-disease-api -p 8000:8000 heart-disease-api:latest
```

### Step 7: Test the API

```bash
# Health check
curl http://localhost:8000/health

# Make a prediction
curl -X POST "http://localhost:8000/predict?age=63&sex=1&cp=3&trestbps=145&chol=233&fbs=1&restecg=0&thalach=150&exang=0&oldpeak=2.3&slope=0&ca=0&thal=1"
```

**Expected Response:**
```json
{
  "prediction": 0,
  "confidence": 0.2737,
  "risk_level": "Low",
  "probability_no_disease": 0.7263,
  "probability_disease": 0.2737,
  "processing_time_ms": 17.38
}
```

### Step 8: View API Documentation

Open in browser: http://localhost:8000/docs

### Step 9: Stop Container (when done)

```bash
docker stop heart-disease-api
docker rm heart-disease-api
```

---

## üì• Data Acquisition

### Download Dataset

The UCI Heart Disease dataset can be downloaded automatically:

```bash
python scripts/download_data.py
```

This script will:
- Download data from UCI Machine Learning Repository
- Clean and preprocess the data
- Save to `data/processed/heart_disease_clean.csv`
- Generate metadata in `data/processed/metadata.json`

### Dataset Description

| Feature | Description | Type |
|---------|-------------|------|
| age | Age in years | Numerical |
| sex | Sex (1=male, 0=female) | Binary |
| cp | Chest pain type (0-3) | Categorical |
| trestbps | Resting blood pressure (mm Hg) | Numerical |
| chol | Serum cholesterol (mg/dl) | Numerical |
| fbs | Fasting blood sugar > 120 mg/dl | Binary |
| restecg | Resting ECG results (0-2) | Categorical |
| thalach | Maximum heart rate achieved | Numerical |
| exang | Exercise induced angina | Binary |
| oldpeak | ST depression induced by exercise | Numerical |
| slope | Slope of peak exercise ST segment | Categorical |
| ca | Number of major vessels (0-3) | Numerical |
| thal | Thalassemia type | Categorical |
| target | Heart disease presence (0/1) | Binary |

---

## üì¶ Model Packaging & Reproducibility

### Saved Model Artifacts

The following files are included in the repository for full reproducibility:

| File | Description |
|------|-------------|
| `models/production/model.pkl` | Trained RandomForestClassifier (pickle format) |
| `models/production/preprocessor.pkl` | ColumnTransformer for feature preprocessing |
| `models/production/model_metadata.json` | Model version, hyperparameters, performance metrics |
| `models/production/feature_names.json` | List of 19 feature names used by the model |
| `models/production/MODEL_CARD.md` | Model documentation and ethical considerations |
| `mlruns/` | MLflow experiment tracking data (parameters, metrics, artifacts) |
| `requirements.txt` | All Python dependencies with pinned versions |
| `src/features/feature_engineering.py` | Feature creation and preprocessing pipeline |

### Preprocessing Pipeline

The preprocessing pipeline (`src/features/feature_engineering.py`) includes:

```python
# Feature Engineering
create_features(df)  # Creates: age_group, chol_cat, bp_cat, hr_reserve, interaction features

# Preprocessing Pipeline
build_preprocessing_pipeline(numerical_features, categorical_features)
# - Numerical: SimpleImputer(median) + StandardScaler
# - Categorical: SimpleImputer(most_frequent) + OneHotEncoder
```

### Reproducibility

- **Random Seed**: 42 (fixed for all training)
- **Python Version**: 3.10+
- **scikit-learn Version**: 1.3.0
- **All dependencies pinned** in `requirements.txt`

### Load Model for Inference

```python
import joblib

# Load model and preprocessor
model = joblib.load('models/production/model.pkl')
preprocessor = joblib.load('models/production/preprocessor.pkl')

# Make prediction
prediction = model.predict(preprocessed_data)
```

---

## üéì Model Training

### Train Models

Train both Logistic Regression and Random Forest models:

```bash
python src/models/train_models.py
```

This will:
1. Load and preprocess the data
2. Train Logistic Regression with hyperparameter tuning
3. Train Random Forest with hyperparameter tuning
4. Log experiments to MLflow
5. Save the best model to `models/production/`

### View MLflow Experiments

```bash
mlflow ui --port 5000
```

Open http://localhost:5000 to view experiment tracking.

---

## üåê API Usage

### Run API Locally

```bash
uvicorn api.app:app --reload --host 0.0.0.0 --port 8000
```

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check |
| `/predict` | POST | Make prediction |
| `/docs` | GET | Swagger documentation |

### Example Request

```bash
curl -X POST "http://localhost:8000/predict?age=63&sex=1&cp=3&trestbps=145&chol=233&fbs=1&restecg=0&thalach=150&exang=0&oldpeak=2.3&slope=0&ca=0&thal=1"
```

### Example Response

```json
{
  "prediction": 1,
  "confidence": 0.85
}
```

### Interactive Documentation

Open http://localhost:8000/docs for Swagger UI.

---

## üê≥ Docker Deployment

### Build Docker Image

```bash
docker build -t heart-disease-api:latest .
```

### Run Container

```bash
docker run -p 8000:8000 heart-disease-api:latest
```

### Using Docker Compose

```bash
docker-compose up -d
```

This starts both the API and MLflow server.

### Test Container

```bash
# Health check
curl http://localhost:8000/health

# Prediction
curl -X POST "http://localhost:8000/predict?age=63&sex=1&cp=3&trestbps=145&chol=233&fbs=1&restecg=0&thalach=150&exang=0&oldpeak=2.3&slope=0&ca=0&thal=1"
```

---

## ‚ò∏Ô∏è Kubernetes Deployment

### Prerequisites

- Minikube or a Kubernetes cluster
- kubectl configured

### Deploy to Minikube

```bash
# Start Minikube
minikube start

# Use Minikube's Docker daemon
eval $(minikube docker-env)

# Build image in Minikube
docker build -t heart-disease-api:latest .

# Apply Kubernetes manifests
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/ingress.yaml

# Get service URL
minikube service heart-disease-api --url
```

### Verify Deployment

```bash
# Check pods
kubectl get pods

# Check services
kubectl get svc

# View logs
kubectl logs -l app=heart-disease-api
```

---

## üîÑ CI/CD Pipeline

The GitHub Actions workflow (`.github/workflows/ci-cd.yml`) includes:

1. **Lint**: Code quality checks with flake8, black, isort
2. **Test**: Unit tests with pytest and coverage
3. **Train**: Model training and artifact generation
4. **Docker**: Build and test Docker image
5. **Integration**: API integration tests
6. **Security**: Bandit and safety vulnerability scans

### Trigger Pipeline

Push to `main` or `develop` branch, or open a pull request.

### View Results

Check the Actions tab in your GitHub repository.

---

## üìä Monitoring

### Prometheus Metrics

The API exposes metrics at `/metrics`:

- `predictions_total`: Total predictions by class
- `prediction_confidence`: Confidence score distribution
- `prediction_latency_seconds`: Prediction latency
- `api_requests_total`: Total API requests

### Start Monitoring Stack

```bash
cd monitoring
docker-compose -f docker-compose-monitoring.yml up -d
```

### Access Dashboards

- Prometheus: http://localhost:9090
- Grafana: http://localhost:3000

### Logs

API logs are written to `logs/api.log`.

---

## üß™ Testing

### Run All Tests

```bash
pytest tests/ -v
```

### Run Unit Tests

```bash
pytest tests/unit/ -v
```

### Run Integration Tests

```bash
# Start the API first
uvicorn api.app:app --port 8000 &

# Run integration tests
pytest tests/integration/ -v
```

### Test Coverage

```bash
pytest tests/ --cov=src --cov=api --cov-report=html
```

---

## üìà MLflow Experiment Tracking

### Features Tracked

- **Parameters**: Hyperparameters, model configuration
- **Metrics**: Accuracy, Precision, Recall, F1, ROC-AUC
- **Artifacts**: Model files, preprocessor, plots

### Start MLflow UI

```bash
mlflow ui --backend-store-uri ./mlruns --port 5000
```

### Compare Experiments

View and compare runs at http://localhost:5000.

---

## üì∏ Generated Screenshots

Running `python scripts/execute_notebooks.py` generates 13 screenshots:

| Notebook | Screenshot | Description |
|----------|------------|-------------|
| 01_eda | `01_class_balance.png` | Target class distribution |
| 01_eda | `01_numerical_histograms.png` | Distribution of numerical features |
| 01_eda | `01_correlation_heatmap.png` | Feature correlation matrix |
| 01_eda | `01_categorical_distributions.png` | Categorical feature distributions |
| 01_eda | `01_boxplots_by_target.png` | Features by target class |
| 02_modeling | `02_model_comparison.png` | Model metrics comparison |
| 02_modeling | `02_roc_curve_comparison.png` | ROC curves for both models |
| 02_modeling | `02_confusion_matrices.png` | Confusion matrices side-by-side |
| 02_modeling | `02_feature_importance.png` | Random Forest feature importance |
| 03_mlflow | `03_lr_confusion_matrix.png` | Logistic Regression confusion matrix |
| 03_mlflow | `03_lr_roc_curve.png` | Logistic Regression ROC curve |
| 03_mlflow | `03_rf_confusion_matrix.png` | Random Forest confusion matrix |
| 03_mlflow | `03_rf_roc_curve.png` | Random Forest ROC curve |

---

## üìù Model Performance

| Model | Accuracy | Precision | Recall | F1-Score | ROC-AUC |
|-------|----------|-----------|--------|----------|---------|
| Logistic Regression | 80.00% | 83.33% | 71.43% | 76.92% | **92.86%** |
| Random Forest | **81.67%** | 81.48% | **78.57%** | **80.00%** | 91.52% |

*Note: Logistic Regression achieved higher ROC-AUC while Random Forest had better overall accuracy.*

---

## üîí Security

- Input validation with Pydantic schemas
- Health checks for container orchestration
- Dependency vulnerability scanning with safety
- Code security analysis with Bandit

---

## üìÑ License

This project is licensed under the MIT License.

---

## üë• Authors

- **Sk Shahrukh Saba** - MLOps Assignment

---

## üôè Acknowledgments

- UCI Machine Learning Repository for the Heart Disease dataset
- BITS Pilani for the MLOps course

